{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pdf Downloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import urllib3\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Suppress only the single InsecureRequestWarning from urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# Function to download a file from a URL\n",
    "def download_file(url, folder):\n",
    "    local_filename = url.split('/')[-1]\n",
    "    local_path = os.path.join(folder, local_filename)\n",
    "    with requests.get(url, stream=True, verify=False) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(local_path, 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "    return local_path\n",
    "\n",
    "# Function to scrape PDF links using BeautifulSoup\n",
    "def scrape_and_download_pdfs(page_url, folder):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(page_url, headers=headers, verify=False)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch {page_url}: {response.status_code}\")\n",
    "        return\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    links = soup.find_all('a', href=True)\n",
    "\n",
    "    # Debug: Print all links\n",
    "    print(f\"Found {len(links)} links on {page_url}\")\n",
    "    for link in links:\n",
    "        print(f\"Text: {link.text.strip()}, Href: {link['href']}\")\n",
    "\n",
    "    # Download PDFs that start with 'summary_' or match the pattern '****report.pdf'\n",
    "    for link in links:\n",
    "        href = link['href']\n",
    "        filename = href.split('/')[-1]\n",
    "        if (filename.startswith('summary_') and filename.endswith('.pdf')) or re.match(r'^\\d{4}report\\.pdf$', filename):\n",
    "            if not href.startswith('http'):\n",
    "                href = 'https://misc.bpdb.gov.bd' + href\n",
    "            print(f\"Found PDF link: {href}\")\n",
    "            download_file(href, folder)\n",
    "\n",
    "# Function to scrape PDFs using Selenium (optional for JavaScript-heavy pages)\n",
    "def scrape_with_selenium(page_url, folder):\n",
    "    driver = webdriver.Chrome()  # Ensure you have ChromeDriver installed\n",
    "    driver.get(page_url)\n",
    "    time.sleep(5)  # Wait for the page to load\n",
    "\n",
    "    links = driver.find_elements(By.TAG_NAME, 'a')\n",
    "    for link in links:\n",
    "        href = link.get_attribute('href')\n",
    "        filename = href.split('/')[-1] if href else ''\n",
    "        if href and ((filename.startswith('summary_') and filename.endswith('.pdf')) or re.match(r'^\\d{4}report\\.pdf$', filename)):\n",
    "            if not href.startswith('http'):\n",
    "                href = 'https://misc.bpdb.gov.bd' + href\n",
    "            print(f\"Found PDF link: {href}\")\n",
    "            download_file(href, folder)\n",
    "    driver.quit()\n",
    "\n",
    "# Main function to iterate over all pages and download PDFs\n",
    "def main():\n",
    "    base_url = 'https://misc.bpdb.gov.bd/daily-generation-archive?page='\n",
    "    download_folder = r'pdf'  # Set the directory where you want to save the PDFs\n",
    "    os.makedirs(download_folder, exist_ok=True)\n",
    "\n",
    "    # Iterate over the pages (adjust range as needed)\n",
    "    for page_num in range(184, 190):\n",
    "        page_url = base_url + str(page_num)\n",
    "        print(f\"Scraping page: {page_url}\")\n",
    "\n",
    "        # Use BeautifulSoup for scraping\n",
    "        scrape_and_download_pdfs(page_url, download_folder)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excel Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize data keys\n",
    "def normalize_keys(data):\n",
    "    key_map = {\n",
    "        \"Max. Demand at eve. peak (Generation end)\": \"Max. Demand at eve. peak (Generation end)\",\n",
    "        \"Max. Demand (Generation end)\": \"Max. Demand at eve. peak (Generation end)\",\n",
    "        \"Max. Demand at eve. peak (Sub-station end)\": \"Max. Demand at eve. peak (Sub-station end)\",\n",
    "        \"Max. Demand (Sub-station end)\": \"Max. Demand at eve. peak (Sub-station end)\",\n",
    "        \"Minimum Generation Forecast up to 8:00 hrs.\": \"Minimum Generation Forecast up to 8:00 hrs.\",\n",
    "        \"Actual Minimum Generation up to 8:00 hrs.\": \"Minimum Generation Forecast up to 8:00 hrs.\",\n",
    "        \"Gas limitation\": \"Gas/LF limitation\",\n",
    "        \"KhMulna_demand\": \"Khulna_demand\",\n",
    "        \"KhMulna_supply\": \"Khulna_supply\",\n",
    "        \"KhMulna_load\": \"Khulna_load\",\n",
    "        \"Maximum Temperature in Dhaka was : 29.6째 C\": \"Maximum Temperature in Dhaka was\",\n",
    "        \"Maximum Temperature in Dhaka was : 29.6째C\": \"Maximum Temperature in Dhaka was\",\n",
    "        \"Maximum Temperature in Dhaka was : 31째 C\": \"Maximum Temperature in Dhaka was\"\n",
    "    }\n",
    "    return {key_map.get(k, k): v for k, v in data.items()}\n",
    "\n",
    "# Function to extract specific data from the text\n",
    "def extract_specific_data(text):\n",
    "    data = {}\n",
    "\n",
    "    # Extract date and day of the week\n",
    "    date_match = re.search(r\"\\(C\\) Actual data of (\\d{2}\\.\\d{2}\\.\\d{2}) \\(\\w+\\) (\\w+)\", text)\n",
    "    if date_match:\n",
    "        data['Actual data of'] = date_match.group(1)\n",
    "        data['Day of the week'] = date_match.group(2)\n",
    "\n",
    "    # Extract various generation values using regular expressions\n",
    "    max_demand_gen_match = re.search(r\"(Max\\. Demand at eve\\. peak \\(Generation end\\)|Max\\. Demand \\(Generation end\\)) : (\\d+\\.?\\d*) MW\", text)\n",
    "    if max_demand_gen_match:\n",
    "        data['Max. Demand at eve. peak (Generation end)'] = float(max_demand_gen_match.group(2))\n",
    "\n",
    "    max_demand_sub_match = re.search(r\"(Max\\. Demand at eve\\. peak \\(Sub-station end\\)|Max\\. Demand \\(Sub-station end\\)) : (\\d+\\.?\\d*) MW\", text)\n",
    "    if max_demand_sub_match:\n",
    "        data['Max. Demand at eve. peak (Sub-station end)'] = float(max_demand_sub_match.group(2))\n",
    "\n",
    "    highest_gen_match = re.search(r\"Highest Generation \\(Generation end\\) : (\\d+\\.?\\d*) MW\", text)\n",
    "    if highest_gen_match:\n",
    "        data['Highest Generation (Generation end)'] = float(highest_gen_match.group(1))\n",
    "\n",
    "    min_gen_match = re.search(r\"Minimum Generation \\(Generation end\\) : (\\d+\\.?\\d*) MW\", text)\n",
    "    if min_gen_match:\n",
    "        data['Minimum Generation (Generation end)'] = float(min_gen_match.group(1))\n",
    "\n",
    "    day_peak_gen_match = re.search(r\"Day-peak Generation \\(Generation end\\) : (\\d+\\.?\\d*) MW\", text)\n",
    "    if day_peak_gen_match:\n",
    "        data['Day-peak Generation (Generation end)'] = float(day_peak_gen_match.group(1))\n",
    "\n",
    "    evening_peak_gen_match = re.search(r\"Evening-peak Generation \\(Generation end\\) : (\\d+\\.?\\d*) MW\", text)\n",
    "    if evening_peak_gen_match:\n",
    "        data['Evening-peak Generation (Generation end)'] = float(evening_peak_gen_match.group(1))\n",
    "\n",
    "    min_gen_forecast_match = re.search(r\"(Minimum Generation Forecast up to 8:00 hrs\\.|Actual Minimum Generation up to 8:00 hrs\\.) : (\\d+\\.?\\d*) MW\", text)\n",
    "    if min_gen_forecast_match:\n",
    "        data['Minimum Generation Forecast up to 8:00 hrs.'] = float(min_gen_forecast_match.group(2))\n",
    "\n",
    "    # Extract temperature with different formats\n",
    "    max_temp_match = re.search(r\"Maximum Temperature in Dhaka was : (\\d+\\.?\\d*)째 ?C\", text)\n",
    "    if max_temp_match:\n",
    "        data['Maximum Temperature in Dhaka was'] = float(max_temp_match.group(1))\n",
    "\n",
    "    gas_limitation_match = re.search(r\"(Gas/LF limitation|Gas limitation) : (\\d+) MW\", text)\n",
    "    if gas_limitation_match:\n",
    "        data['Gas/LF limitation'] = int(gas_limitation_match.group(2))\n",
    "\n",
    "    coal_supply_limitation_match = re.search(r\"Coal supply Limitation : (\\d+) MW\", text)\n",
    "    if coal_supply_limitation_match:\n",
    "        data['Coal supply Limitation'] = int(coal_supply_limitation_match.group(1))\n",
    "\n",
    "    water_level_limitation_match = re.search(r\"Low water level in Kaptai lake : (\\d+) MW\", text)\n",
    "    if water_level_limitation_match:\n",
    "        data['Low water level in Kaptai lake'] = int(water_level_limitation_match.group(1))\n",
    "\n",
    "    plants_shutdown_match = re.search(r\"Plants under shut down/ maintenance : (\\d+) MW\", text)\n",
    "    if plants_shutdown_match:\n",
    "        data['Plants under shut down/ maintenance'] = int(plants_shutdown_match.group(1))\n",
    "\n",
    "    # Extract demand, supply, and load shed for each zone\n",
    "    zones = ['Dhaka', 'Chattogram', 'Khulna', 'Rajshahi', 'Mymensingh', 'Sylhet', 'Barishal', 'Rangpur', 'Cumilla']\n",
    "    for zone in zones:\n",
    "        zone_match = re.search(rf\"{zone} (\\d+) (\\d+) (\\d+)\", text)\n",
    "        if zone_match:\n",
    "            data[f'{zone}_demand'] = int(zone_match.group(1))\n",
    "            data[f'{zone}_supply'] = int(zone_match.group(2))\n",
    "            data[f'{zone}_load'] = int(zone_match.group(3))\n",
    "\n",
    "    # Handle the case where Khulna is misspelled as KhMulna\n",
    "    zone_match = re.search(r\"KhMulna (\\d+) (\\d+) (\\d+)\", text)\n",
    "    if zone_match:\n",
    "        data['Khulna_demand'] = int(zone_match.group(1))\n",
    "        data['Khulna_supply'] = int(zone_match.group(2))\n",
    "        data['Khulna_load'] = int(zone_match.group(3))\n",
    "\n",
    "    return normalize_keys(data)\n",
    "\n",
    "# Directory containing PDF files\n",
    "pdf_dir = r\"pdf\" #set the directory where the PDFs are stored\n",
    "\n",
    "# Path to the Excel file\n",
    "excel_path = r\"data.xlsx\" #set the path where you want to save the Excel file\n",
    "\n",
    "# Path to the error log file\n",
    "error_log_path = r\"errors.txt\" #set the path where you want to save the error log file\n",
    "\n",
    "# Predefine the expected columns to ensure they are always present\n",
    "expected_columns = [\n",
    "    \"Actual data of\", \"Day of the week\", \"Max. Demand at eve. peak (Generation end)\",\n",
    "    \"Max. Demand at eve. peak (Sub-station end)\", \"Highest Generation (Generation end)\",\n",
    "    \"Minimum Generation (Generation end)\", \"Day-peak Generation (Generation end)\",\n",
    "    \"Evening-peak Generation (Generation end)\", \"Minimum Generation Forecast up to 8:00 hrs.\",\n",
    "    \"Maximum Temperature in Dhaka was\", \"Gas/LF limitation\", \"Coal supply Limitation\", \n",
    "    \"Low water level in Kaptai lake\", \"Plants under shut down/ maintenance\"\n",
    "]\n",
    "zones = ['Dhaka', 'Chattogram', 'Khulna', 'Rajshahi', 'Mymensingh', 'Sylhet', 'Barishal', 'Rangpur', 'Cumilla']\n",
    "for zone in zones:\n",
    "    expected_columns.extend([f'{zone}_demand', f'{zone}_supply', f'{zone}_load'])\n",
    "\n",
    "# Initialize the error log\n",
    "with open(error_log_path, 'w') as error_log:\n",
    "    error_log.write(\"Error Log\\n\")\n",
    "\n",
    "# Process each PDF file one by one\n",
    "for filename in os.listdir(pdf_dir):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        pdf_path = os.path.join(pdf_dir, filename)\n",
    "        try:\n",
    "            with pdfplumber.open(pdf_path) as pdf:\n",
    "                # Extract text from page 2 (pages are 0-indexed, so page 2 is index 1)\n",
    "                page = pdf.pages[1]  # Page 2 corresponds to index 1\n",
    "                text = page.extract_text()\n",
    "                # Extract specific data from the text\n",
    "                data = extract_specific_data(text)\n",
    "                \n",
    "                # Ensure all expected columns are present\n",
    "                for col in expected_columns:\n",
    "                    if col not in data:\n",
    "                        data[col] = None\n",
    "\n",
    "                # Convert the extracted data into a DataFrame\n",
    "                df = pd.DataFrame([data])\n",
    "\n",
    "                # Append data to the Excel file\n",
    "                if os.path.exists(excel_path):\n",
    "                    existing_df = pd.read_excel(excel_path)\n",
    "                    combined_df = pd.concat([existing_df, df], ignore_index=True)\n",
    "                    combined_df.to_excel(excel_path, index=False)\n",
    "                else:\n",
    "                    df.to_excel(excel_path, index=False)\n",
    "\n",
    "                print(f\"Data from {filename} has been successfully extracted and saved to the Excel file.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            # Log the error and continue with the next file\n",
    "            with open(error_log_path, 'a') as error_log:\n",
    "                error_log.write(f\"Error processing {filename}: {str(e)}\\n\")\n",
    "            print(f\"Error processing {filename}: {str(e)}\")\n",
    "\n",
    "print(f\"Data extraction and saving completed. Errors, if any, have been logged to {error_log_path}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
